# -*- coding: utf-8 -*-
"""Vehicle_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CBT-NiiJIAI8RCFzKPvUC3rjRLEl9Y98

# Step 1: Basic Setup
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# ML
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

# Display all columns
pd.set_option('display.max_columns', None)

# Upload CSV (Manual for now)
from google.colab import files
uploaded = files.upload()

# Load the dataset
df = pd.read_csv(list(uploaded.keys())[0])
df.head()

"""STEP 2: Initial Exploration + Data Cleaning"""

# Step 2: Quick Exploration
print("Shape of dataset:", df.shape)
print("\nData types:\n", df.dtypes)
print("\nMissing values:\n", df.isnull().sum())

# Check basic stats
df.describe(include='all').T

"""STEP 3: Data Cleaning + Feature Engineering"""

# Step 3: Data Cleaning

# Drop irrelevant or redundant columns
df.drop(['name', 'description'], axis=1, inplace=True)

# Drop rows where price or year is missing
df = df.dropna(subset=['price', 'year'])

# Fill remaining missing values
df['cylinders'] = df['cylinders'].fillna(df['cylinders'].mode()[0])
df['mileage'] = df['mileage'].fillna(df['mileage'].median())
df['doors'] = df['doors'].fillna(df['doors'].mode()[0])
df['trim'] = df['trim'].fillna("Unknown")

# Drop rows with any still-missing values (if few left)
df.dropna(inplace=True)

# Encode categorical features
categorical_cols = df.select_dtypes(include='object').columns

le = LabelEncoder()
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

# Final shape check
print("Data shape after cleaning:", df.shape)
df.head()

"""STEP 4: Model Building – Linear Regression & XGBoost"""

from sklearn.linear_model import LinearRegression

# Step 4: Train/Test Split
X = df.drop('price', axis=1)
y = df['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Model 1: Linear Regression
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)
y_pred_lr = lr.predict(X_test_scaled)

# Model 2: XGBoost Regressor
xgbr = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
xgbr.fit(X_train, y_train)
y_pred_xgb = xgbr.predict(X_test)

# Evaluation Function
def evaluate_model(y_true, y_pred, name):
    print(f"{name} Performance:")
    print("MAE:", mean_absolute_error(y_true, y_pred))
    print("R² Score:", r2_score(y_true, y_pred))
    print("-" * 30)

# Evaluate both models
evaluate_model(y_test, y_pred_lr, "Linear Regression")
evaluate_model(y_test, y_pred_xgb, "XGBoost")

""" STEP 5: Final Touches"""

# Step 5: Visualize Predictions
plt.figure(figsize=(8, 5))
sns.scatterplot(x=y_test, y=y_pred_xgb)
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("XGBoost: Actual vs Predicted Price")
plt.grid(True)
plt.show()

# Export Predictions
results_df = pd.DataFrame({'Actual Price': y_test, 'Predicted Price': y_pred_xgb})
results_df.to_csv("vehicle_price_predictions.csv", index=False)

# Optional: Save XGBoost model
import joblib
joblib.dump(xgbr, "xgboost_vehicle_price_model.pkl")